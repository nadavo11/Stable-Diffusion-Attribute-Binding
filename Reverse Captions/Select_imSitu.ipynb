{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1T51HgMhTGBYJ7gu8Q3wVj9OQDa-f6CYV","timestamp":1723562304476}],"authorship_tag":"ABX9TyNo6Tlhr/h0A6lfTzv/QJLL"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["##### imSitu images can be downloaded from here: https://prior.allenai.org/projects/imsitu"],"metadata":{"id":"F8uG3CJfcOMF"}},{"cell_type":"code","source":["import json\n","import regex as re\n","import pandas as pd\n","import itertools\n","import os\n","from collections import Counter\n","from google.colab import drive"],"metadata":{"id":"Q1xvJ5WZhuZy","executionInfo":{"status":"ok","timestamp":1726163383560,"user_tz":-180,"elapsed":348,"user":{"displayName":"Lia Baron","userId":"15103268211156396612"}}},"execution_count":32,"outputs":[]},{"cell_type":"code","source":["# Mount\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P0vaW5gIVJZz","executionInfo":{"status":"ok","timestamp":1726163387318,"user_tz":-180,"elapsed":2405,"user":{"displayName":"Lia Baron","userId":"15103268211156396612"}},"outputId":"b18a1a45-1aaf-4b2e-cd4d-764af82841d2"},"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["# In the working dir you should have the following files (from imSitu dataset):\n","# train.json\n","# dev.json\n","# test.json\n","# imsitu_space.json\n","working_dir = '/content/drive/My Drive/deep_learning_project/imSitu/'"],"metadata":{"id":"1d27I08QXbWA","executionInfo":{"status":"ok","timestamp":1726163401843,"user_tz":-180,"elapsed":334,"user":{"displayName":"Lia Baron","userId":"15103268211156396612"}}},"execution_count":34,"outputs":[]},{"cell_type":"code","source":["def fix_articles(sentence):\n","    # Define a regex pattern to capture the articles \"a\" or \"an\" followed by a word\n","    pattern = r'\\b(a|an)\\s+([a-zA-Z]+)'\n","\n","    # Function to check if a word starts with a vowel sound\n","    def correct_article(match):\n","        article = match.group(1)\n","        word = match.group(2)\n","\n","        # List of vowel sounds\n","        vowels = 'aeiouAEIOU'\n","\n","        # Determine if the word starts with a vowel\n","        if word[0] in vowels:\n","            # If the article is \"a\" but it should be \"an\"\n","            if article == 'a':\n","                return 'an ' + word\n","        else:\n","            # If the article is \"an\" but it should be \"a\"\n","            if article == 'an':\n","                return 'a ' + word\n","\n","        # Return the original article and word if no correction is needed\n","        return article + ' ' + word\n","\n","    # Use the sub function to replace incorrect \"a\"/\"an\" with the correct one\n","    corrected_sentence = re.sub(pattern, correct_article, sentence)\n","\n","    return corrected_sentence"],"metadata":{"id":"55BSlhuuPw5L","executionInfo":{"status":"ok","timestamp":1726163404196,"user_tz":-180,"elapsed":362,"user":{"displayName":"Lia Baron","userId":"15103268211156396612"}}},"execution_count":35,"outputs":[]},{"cell_type":"code","source":["with open(working_dir + 'imsitu_space.json', 'r') as file:\n","    # Load the content of the file into a dictionary\n","    imsitu_space_dict = json.load(file)\n","with open(working_dir + 'train.json') as file:\n","  imsitu_train_dict = json.load(file)\n","with open(working_dir + 'dev.json') as file:\n","  imsitu_dev_dict = json.load(file)\n","with open(working_dir + 'test.json') as file:\n","  imsitu_test_dict = json.load(file)\n","imsitu_dataset_dict = {**imsitu_train_dict, **imsitu_dev_dict, **imsitu_test_dict}\n","del imsitu_train_dict, imsitu_dev_dict, imsitu_test_dict"],"metadata":{"id":"-Ku7AmHopbp8","executionInfo":{"status":"ok","timestamp":1726163408313,"user_tz":-180,"elapsed":2060,"user":{"displayName":"Lia Baron","userId":"15103268211156396612"}}},"execution_count":36,"outputs":[]},{"cell_type":"code","source":["df_sorted = pd.DataFrame(columns=[\"file_name\", \"text\"])\n","os.makedirs(f\"{working_dir}data/\", exist_ok=True)\n","\n","# Regex pattern to extract the part before the underscore\n","pattern = r'^[^_]+'\n","\n","for image_key in imsitu_dataset_dict.keys():\n","    group_key = re.match(pattern, image_key).group()\n","    frames_list = imsitu_dataset_dict[image_key]['frames']\n","    combined_dict = {}\n","\n","    for index, frame in enumerate(frames_list):\n","        for annot, noun_key in frame.items():\n","            gloss_value = []\n","            if noun_key:\n","                gloss_value = imsitu_space_dict['nouns'][noun_key]['gloss']\n","\n","            # Combine gloss values directly into the final dict\n","            if annot not in combined_dict:\n","                combined_dict[annot] = set()  # Use a set to ensure uniqueness\n","            combined_dict[annot].update(gloss_value)\n","\n","    # Convert sets to sorted lists\n","    for annot in combined_dict:\n","        combined_dict[annot] = sorted(list(combined_dict[annot]))\n","\n","    # Adding the abstract directly\n","    combined_dict['abstract'] = imsitu_space_dict['verbs'][group_key]['abstract']\n","\n","    # Convert the dictionary to a DataFrame and store it in dfs_dict\n","    tmp_df = pd.DataFrame([combined_dict])\n","\n","    # Extract all columns except 'abstract'\n","    columns = [col for col in tmp_df.columns if col != \"abstract\"]\n","\n","    # Create a mapping from uppercase placeholders to actual columns\n","    words_to_replace = {col.upper(): tmp_df[col].values[0] for col in columns}\n","\n","    # Create all possible combinations\n","    combinations = list(itertools.product(*words_to_replace.values()))\n","\n","    # Generate sentences for all combinations\n","    descriptions = []\n","    for combination in combinations:\n","        sentence = combined_dict['abstract']\n","        for i, placeholder in enumerate(words_to_replace.keys()):\n","            # Convert list to a string before replacing\n","            sentence = sentence.replace(placeholder, combination[i])\n","        descriptions.append(sentence)\n","    tmp_df[\"descriptions\"] = [descriptions]\n","    if len(tmp_df[\"descriptions\"][0]) > 0:\n","      # Initialize a Counter to track the occurrence of each value across different columns\n","      value_counter = Counter()\n","      # Iterate over each column's list and update the counter\n","      for column in tmp_df.columns:\n","          if column not in [\"abstract\", \"descriptions\"]:\n","            value_counter.update(set(tmp_df.iloc[0][column]))  # Convert list to set to ignore duplicates within a single column\n","      # Check for values that appear in more than one list (i.e., have a count > 1)\n","      duplicates = [value for value, count in value_counter.items() if count > 1]\n","      if len(duplicates) == 0:\n","        caption = tmp_df[\"descriptions\"][0][0]\n","        caption = fix_articles(caption)\n","        caption = caption.capitalize()\n","        new_data = {\"file_name\": f\"{working_dir}data/full_resized_images_dataset/of500_images_resized/{image_key}\", \"text\": caption}\n","        # Convert the new data to a DataFrame and concatenate it\n","        df_sorted = pd.concat([df_sorted, pd.DataFrame([new_data])], ignore_index=True)\n","\n","df_sorted.to_csv(f\"{working_dir}data/output.csv\", index=False)\n","#del imsitu_dataset_dict\n"],"metadata":{"id":"cfSZDaesIGtz"},"execution_count":null,"outputs":[]}]}